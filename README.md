# llm-perf

Build a simulation or profiler-based model predicting inference latency for different batching, sequence lengths, and GPU configs.
